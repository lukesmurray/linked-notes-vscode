
@article{andyWhyBooksDon2019,
  title = {Why Books Don't Work},
  author = {Andy, Matuschak},
  year = {2019},
  abstract = {Designing media to reflect how people think and learn},
  file = {/Users/lukemurray/Zotero/storage/GSRFXN6N/books.html}
}

@inproceedings{banovicWakenReverseEngineering2012,
  title = {Waken: Reverse Engineering Usage Information and Interface Structure from Software Videos},
  shorttitle = {Waken},
  booktitle = {Proceedings of the 25th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology - {{UIST}} '12},
  author = {Banovic, Nikola and Grossman, Tovi and Matejka, Justin and Fitzmaurice, George},
  year = {2012},
  pages = {83},
  publisher = {{ACM Press}},
  address = {{Cambridge, Massachusetts, USA}},
  doi = {10.1145/2380116.2380129},
  abstract = {We present Waken, an application-independent system that recognizes UI components and activities from screen captured videos, without any prior knowledge of that application. Waken can identify the cursors, icons, menus, and tooltips that an application contains, and when those items are used. Waken uses frame differencing to identify occurrences of behaviors that are common across graphical user interfaces. Candidate templates are built, and then other occurrences of those templates are identified using a multiphase algorithm. An evaluation demonstrates that the system can successfully reconstruct many aspects of a UI without any prior application-dependant knowledge. To showcase the design opportunities that are introduced by having this additional meta-data, we present the Waken Video Player, which allows users to directly interact with UI components that are displayed in the video.},
  file = {/Users/lukemurray/Zotero/storage/J5XZQLMN/Banovic et al. - 2012 - Waken reverse engineering usage information and i.pdf},
  isbn = {978-1-4503-1580-7},
  language = {en}
}

@book{batesTeachingDigitalAge2019,
  title = {Teaching in a {{Digital Age}} - {{Second Edition}}},
  author = {Bates, A. W. (Tony)},
  year = {2019},
  month = oct,
  publisher = {{Tony Bates Associates Ltd.}},
  file = {/Users/lukemurray/Zotero/storage/3EJHAP2L/teachinginadigitalagev2.html},
  language = {en}
}

@inproceedings{beaudouin-lafonInstrumentalInteractionInteraction2000,
  title = {Instrumental Interaction: An Interaction Model for Designing Post-{{WIMP}} User Interfaces},
  shorttitle = {Instrumental Interaction},
  booktitle = {Proceedings of the {{SIGCHI}} Conference on {{Human}} Factors in Computing Systems  - {{CHI}} '00},
  author = {{Beaudouin-Lafon}, Michel},
  year = {2000},
  pages = {446--453},
  publisher = {{ACM Press}},
  address = {{The Hague, The Netherlands}},
  doi = {10.1145/332040.332473},
  abstract = {This article introduces a new interaction model called Instrumental Interaction that extends and generalizes the principles of direct manipulation. It covers existing interaction styles, including traditional WIMP interfaces, as well as new interaction styles such as two-handed input and augmented reality. It defines a design space for new interaction techniques and a set of properties for comparing them. Instrumental Interaction describes graphical user interfaces in terms of domain objects and interaction instruments. Interaction between users and domain objects is mediated by interaction instruments, similar to the tools and instruments we use in the real world to interact with physical objects. The article presents the model, applies it to describe and compare a number of interaction techniques, and shows how it was used to create a new interface for searching and replacing text.},
  file = {/Users/lukemurray/Zotero/storage/ZHNNI8VP/Beaudouin-Lafon - 2000 - Instrumental interaction an interaction model for.pdf},
  isbn = {978-1-58113-216-8},
  language = {en}
}

@article{beaudouin-lafonReificationPolymorphismReuse,
  title = {Reification, {{Polymorphism}} and {{Reuse}}: {{Three Principles}} for {{Designing Visual Interfaces}}},
  author = {{Beaudouin-Lafon}, Michel and Mackay, Wendy E},
  pages = {8},
  abstract = {This paper presents three design principles to support the development of large-scale applications and take advantage of recent research in new interaction techniques: Reification turns concepts into first class objects, polymorphism permits commands to be applied to objects of different types, and reuse makes both user input and system output accessible for later use. We show that the power of these principles lies in their combination. Reification creates new objects that can be acted upon by a small set of polymorphic commands, creating more opportunities for reuse. The result is a simpler yet more powerful interface.},
  file = {/Users/lukemurray/Zotero/storage/SHKTS43K/Beaudouin-Lafon and Mackay - Reification, Polymorphism and Reuse Three Princip.pdf},
  language = {en}
}

@inproceedings{beaudouin-lafonUnifiedPrinciplesInteraction2017,
  title = {Towards {{Unified Principles}} of {{Interaction}}},
  booktitle = {{{CHItaly}} 2017 - {{Proceedings}} of the 12th {{Biannual Conference}} of the {{Italian SIGCHI Chapter}}},
  author = {{Beaudouin-Lafon}, Michel},
  year = {2017},
  month = sep,
  pages = {1--2},
  address = {{Cagliari, Italy}},
  doi = {10.1145/3125571.3125602},
  abstract = {Even though today's computers are used for many different types of tasks, they still rely on user interfaces designed for office workers in the 1980s. HCI researchers have produced a slew of innovative interaction styles, from gestural interaction to mixed reality and tangible interfaces, but they have not replaced traditional GUIs. I argue that we must devise fundamental principles of interaction that unify, rather than separate, interaction styles in order to support the diversity of uses and users. I describe ongoing work on my ERC advanced grant, ONE, which explores how the concepts of information substrates and interaction instruments create digital environments that users can appropriate and (re)combine at will.},
  file = {/Users/lukemurray/Zotero/storage/ZI86J3NI/Beaudouin-Lafon - 2017 - Towards Unified Principles of Interaction.pdf},
  keywords = {arvind,Conceptual Model,Information Substrates,Instrumental Interaction,Interaction Principles,semantic desktop}
}

@article{brehmerTimelinesRevisitedDesign2017,
  title = {Timelines {{Revisited}}: {{A Design Space}} and {{Considerations}} for {{Expressive Storytelling}}},
  shorttitle = {Timelines {{Revisited}}},
  author = {Brehmer, Matthew and Lee, Bongshin and Bach, Benjamin and Riche, Nathalie Henry and Munzner, Tamara},
  year = {2017},
  month = sep,
  volume = {23},
  pages = {2151--2164},
  issn = {1077-2626},
  doi = {10.1109/TVCG.2016.2614803},
  abstract = {There are many ways to visualize event sequences as timelines. In a storytelling context where the intent is to convey multiple narrative points, a richer set of timeline designs may be more appropriate than the narrow range that has been used for exploratory data analysis by the research community. Informed by a survey of 263 timelines, we present a design space for storytelling with timelines that balances expressiveness and effectiveness, identifying 14 design choices characterized by three dimensions: representation, scale, and layout. Twenty combinations of these choices are viable timeline designs that can be matched to different narrative points, while smooth animated transitions between narrative points allow for the presentation of a cohesive story, an important aspect of both interactive storytelling and data videos. We further validate this design space by realizing the full set of viable timeline designs and transitions in a proof-of-concept sandbox implementation that we used to produce seven example timeline stories. Ultimately, this work is intended to inform and inspire the design of future tools for storytelling with timelines.},
  file = {/Users/lukemurray/Zotero/storage/MQZCIEQ5/Brehmer et al. - 2017 - Timelines Revisited A Design Space and Considerat.pdf},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  language = {en},
  number = {9}
}

@inproceedings{buntTaggedCommentsPromotingIntegrating2014,
  title = {{{TaggedComments}}: Promoting and Integrating User Comments in Online Application Tutorials},
  shorttitle = {{{TaggedComments}}},
  booktitle = {Proceedings of the 32nd Annual {{ACM}} Conference on {{Human}} Factors in Computing Systems - {{CHI}} '14},
  author = {Bunt, Andrea and Dubois, Patrick and Lafreniere, Ben and Terry, Michael A. and Cormack, David T.},
  year = {2014},
  pages = {4037--4046},
  publisher = {{ACM Press}},
  address = {{Toronto, Ontario, Canada}},
  doi = {10.1145/2556288.2557118},
  abstract = {User comments posted to popular online tutorials constitute a rich additional source of information for readers, yet current designs for displaying user comments on tutorial webpages do little to support their use. Instead, comments are separated from the tutorial content they reference and tend to be ordered according to post date. We propose and evaluate the TaggedComments system, a new approach to displaying comments that users post to online tutorials. Using tags supplied by commenters, TaggedComments seeks to enhance the role of user comments by 1) improving their visibility, 2) allowing users to personalize their use of the comments according to their particular information needs, and 3) providing direct access to potentially helpful comments from the tutorial content. A laboratory evaluation with 16 participants shows that, in comparison to the standard comment layout, TaggedComments significantly improves users' subjective impressions of comment utility when interacting with Photoshop tutorials.},
  file = {/Users/lukemurray/Zotero/storage/FI3QCHZX/Bunt et al. - 2014 - TaggedComments promoting and integrating user com.pdf},
  isbn = {978-1-4503-2473-1},
  language = {en}
}

@inproceedings{callahanManagingEvolutionDataflows2006,
  title = {Managing the {{Evolution}} of {{Dataflows}} with {{VisTrails}}},
  booktitle = {22nd {{International Conference}} on {{Data Engineering Workshops}} ({{ICDEW}}'06)},
  author = {Callahan, S.P. and Freire, J. and Santos, E. and Scheidegger, C.E. and Silva, C.T. and {Huy T. Vo}},
  year = {2006},
  pages = {71--71},
  publisher = {{IEEE}},
  address = {{Atlanta, GA, USA}},
  doi = {10.1109/ICDEW.2006.75},
  file = {/Users/lukemurray/Zotero/storage/FIXM3MTV/Callahan et al. - 2006 - Managing the Evolution of Dataflows with VisTrails.pdf},
  isbn = {978-0-7695-2571-6},
  language = {en}
}

@inproceedings{chiDemoWizReperformingSoftware2014,
  title = {{{DemoWiz}}: Re-Performing Software Demonstrations for a Live Presentation},
  shorttitle = {{{DemoWiz}}},
  booktitle = {Proceedings of the 32nd Annual {{ACM}} Conference on {{Human}} Factors in Computing Systems - {{CHI}} '14},
  author = {Chi, Pei-Yu and Lee, Bongshin and Drucker, Steven M.},
  year = {2014},
  pages = {1581--1590},
  publisher = {{ACM Press}},
  address = {{Toronto, Ontario, Canada}},
  doi = {10.1145/2556288.2557254},
  abstract = {Showing a live software demonstration during a talk can be engaging, but it is often not easy: presenters may struggle with (or worry about) unexpected software crashes and encounter issues such as mismatched screen resolutions or faulty network connectivity. Furthermore, it can be difficult to recall the steps to show while talking and operating the system all at the same time. An alternative is to present with pre-recorded screencast videos. It is, however, challenging to precisely match the narration to the video when using existing video players. We introduce DemoWiz, a video presentation system that provides an increased awareness of upcoming actions through glanceable visualizations. DemoWiz supports better control of timing by overlaying visual cues and enabling lightweight editing. A user study shows that our design significantly improves the presenters' perceived ease of narration and timing compared to a system without visualizations that was similar to a standard playback control. Furthermore, nine (out of ten) participants preferred DemoWiz over the standard playback control with the last expressing no preference.},
  file = {/Users/lukemurray/Zotero/storage/FN44XXXS/FN44XXXS.pdf},
  isbn = {978-1-4503-2473-1},
  language = {en}
}

@inproceedings{chilanaLemonAidSelectionbasedCrowdsourced2012,
  title = {{{LemonAid}}: Selection-Based Crowdsourced Contextual Help for Web Applications},
  shorttitle = {{{LemonAid}}},
  booktitle = {Proceedings of the 2012 {{ACM}} Annual Conference on {{Human Factors}} in {{Computing Systems}} - {{CHI}} '12},
  author = {Chilana, Parmit K. and Ko, Andrew J. and Wobbrock, Jacob O.},
  year = {2012},
  pages = {1549},
  publisher = {{ACM Press}},
  address = {{Austin, Texas, USA}},
  doi = {10.1145/2207676.2208620},
  abstract = {Web-based technical support such as discussion forums and social networking sites have been successful at ensuring that most technical support questions eventually receive helpful answers. Unfortunately, finding these answers is still quite difficult, since users' textual queries are often incomplete, imprecise, or use different vocabularies to describe the same problem. We present LemonAid, a new approach to help that allows users to find help by instead selecting a label, widget, link, image or other user interface (UI) element that they believe is relevant to their problem. LemonAid uses this selection to retrieve previously asked questions and their corresponding answers. The key insight that makes LemonAid work is that users tend to make similar selections in the interface for similar help needs and different selections for different help needs. Our initial evaluation shows that across a corpus of dozens of tasks and thousands of requests, LemonAid retrieved a result for 90\% of help requests based on UI selections and, of those, over half had relevant matches in the top 2 results.},
  file = {/Users/lukemurray/Zotero/storage/HCIRBXUW/Chilana et al. - 2012 - LemonAid selection-based crowdsourced contextual .pdf},
  isbn = {978-1-4503-1015-4},
  language = {en}
}

@book{derthickEnhancingDataExploration2001,
  title = {Enhancing {{Data Exploration}} with a {{Branching History}} of {{User Operations}}},
  author = {Derthick, Mark and Roth, Steven F.},
  year = {2001},
  abstract = {Backtracking and investigating alternative scenarios are a integral parts of exploratory data analysis. Yet today's interfaces cannot represent alternative exploration paths as a branching history, forcing the user to recognize conceptual branch points in a linear history. Further, the interface can only show information from one state at a time, forcing users to rely on memory to compare scenarios. Our system includes a tree-structured visualization for navigating across time and scenarios. The visualization also allows browsing the history and selectively undoing/redoing events within a scenario or across scenarios. It uses the AI formalism of contexts to maintain multiple, possibly mutually inconsistent, knowledge base states. Cross-context formulas can be written for explicit scenario comparison, including visualizations of scenario differences.},
  file = {/Users/lukemurray/Zotero/storage/7BR2BE3M/Derthick and Roth - 2001 - Enhancing Data Exploration with a Branching Histor.pdf;/Users/lukemurray/Zotero/storage/KCAJ63IP/summary.html}
}

@inproceedings{dixonPrefabImplementingAdvanced2010,
  title = {Prefab: Implementing Advanced Behaviors Using Pixel-Based Reverse Engineering of Interface Structure},
  shorttitle = {Prefab},
  booktitle = {Proceedings of the 28th International Conference on {{Human}} Factors in Computing Systems - {{CHI}} '10},
  author = {Dixon, Morgan and Fogarty, James},
  year = {2010},
  pages = {1525},
  publisher = {{ACM Press}},
  address = {{Atlanta, Georgia, USA}},
  doi = {10.1145/1753326.1753554},
  abstract = {Current chasms between applications implemented with different user interface toolkits make it difficult to implement and explore potentially important interaction techniques in new and existing applications, limiting the progress and impact of human-computer interaction research. We examine an approach based in the single most common characteristic of all graphical user interface toolkits, that they ultimately paint pixels to a display. We present Prefab, a system for implementing advanced behaviors through the reverse engineering of the pixels in graphical interfaces. Informed by how user interface toolkits paint interfaces, Prefab features a separation of the modeling of widget layout from the recognition of widget appearance. We validate Prefab in implementations of three applications: target-aware pointing techniques, Phosphor transitions, and Side Views parameter spectrums. Working only from pixels, we demonstrate a single implementation of these enhancements in complex existing applications created in different user interface toolkits running on different windowing systems.},
  file = {/Users/lukemurray/Zotero/storage/M4R9FXAV/Dixon and Fogarty - 2010 - Prefab implementing advanced behaviors using pixe.pdf},
  isbn = {978-1-60558-929-9},
  language = {en}
}

@inproceedings{drososWrexUnifiedProgrammingbyExample2020,
  title = {Wrex: {{A Unified Programming}}-by-{{Example Interaction}} for {{Synthesizing Readable Code}} for {{Data Scientists}}},
  shorttitle = {Wrex},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Drosos, Ian and Barik, Titus and Guo, Philip J. and DeLine, Robert and Gulwani, Sumit},
  year = {2020},
  month = apr,
  pages = {1--12},
  publisher = {{ACM}},
  address = {{Honolulu HI USA}},
  doi = {10.1145/3313831.3376442},
  abstract = {Data wrangling is a diffcult and time-consuming activity in computational notebooks, and existing wrangling tools do not ft the exploratory workfow for data scientists in these environments. We propose a unifed interaction model based on programming-by-example that generates readable code for a variety of useful data transformations, implemented as a Jupyter notebook extension called WREX. User study results demonstrate that data scientists are signifcantly more effective and effcient at data wrangling with WREX over manual programming. Qualitative participant feedback indicates that WREX was useful and reduced barriers in having to recall or look up the usage of various data transform functions. The synthesized code allowed data scientists to verify the intended data transformation, increased their trust and confdence in WREX, and ft seamlessly within their cell-based notebook workfows. This work suggests that presenting readable code to professional data scientists is an indispensable component of offering data wrangling tools in notebooks.},
  file = {/Users/lukemurray/Zotero/storage/IN3WCAJ2/Drosos et al. - 2020 - Wrex A Unified Programming-by-Example Interaction.pdf},
  isbn = {978-1-4503-6708-0},
  language = {en}
}

@inproceedings{ekstrandSearchingSoftwareLearning2011,
  title = {Searching for Software Learning Resources Using Application Context},
  booktitle = {Proceedings of the 24th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology - {{UIST}} '11},
  author = {Ekstrand, Michael and Li, Wei and Grossman, Tovi and Matejka, Justin and Fitzmaurice, George},
  year = {2011},
  pages = {195},
  publisher = {{ACM Press}},
  address = {{Santa Barbara, California, USA}},
  doi = {10.1145/2047196.2047220},
  abstract = {Users of complex software applications frequently need to consult documentation, tutorials, and support resources to learn how to use the software and further their understanding of its capabilities. Existing online help systems provide limited context awareness through ``what's this?'' and similar techniques. We examine the possibility of making more use of the user's current context in a particular application to provide useful help resources. We provide an analysis and taxonomy of various aspects of application context and how they may be used in retrieving software help artifacts with web browsers, present the design of a context-aware augmented web search system, and describe a prototype implementation and initial user study of this system. We conclude with a discussion of open issues and an agenda for further research.},
  file = {/Users/lukemurray/Zotero/storage/7L2N85SR/Ekstrand et al. - 2011 - Searching for software learning resources using ap.pdf},
  isbn = {978-1-4503-0716-1},
  language = {en}
}

@inproceedings{fourneyMiningOnlineSoftware2014,
  title = {Mining Online Software Tutorials: Challenges and Open Problems},
  shorttitle = {Mining Online Software Tutorials},
  booktitle = {Proceedings of the Extended Abstracts of the 32nd Annual {{ACM}} Conference on {{Human}} Factors in Computing Systems - {{CHI EA}} '14},
  author = {Fourney, Adam and Terry, Michael},
  year = {2014},
  pages = {653--664},
  publisher = {{ACM Press}},
  address = {{Toronto, Ontario, Canada}},
  doi = {10.1145/2559206.2578862},
  abstract = {Web-based software tutorials contain a wealth of information describing software tasks and workflows. There is growing interest in mining these resources for task modeling, automation, machine-guided help, interface search, and other applications. As a first step, past work has shown success in extracting individual commands from textual instructions. In this paper, we ask: How much further do we have to go to more fully interpret or automate a tutorial? We take a bottom-up approach, asking what it would take to: (1) interpret individual steps, (2) follow sequences of steps, and (3) locate procedural content in larger texts.},
  file = {/Users/lukemurray/Zotero/storage/7E6S95ZL/Fourney and Terry - 2014 - Mining online software tutorials challenges and o.pdf},
  isbn = {978-1-4503-2474-8},
  language = {en}
}

@article{grablerGeneratingPhotoManipulation2009,
  ids = {grablerGeneratingPhotoManipulation2009a},
  title = {Generating Photo Manipulation Tutorials by Demonstration},
  author = {Grabler, Floraine and Agrawala, Maneesh and Li, Wilmot and Dontcheva, Mira and Igarashi, Takeo},
  year = {2009},
  month = jul,
  volume = {28},
  pages = {1--9},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/1531326.1531372},
  abstract = {We present a demonstration-based system for automatically generating succinct step-by-step visual tutorials of photo manipulations. An author first demonstrates the manipulation using an instrumented version of GIMP that records all changes in interface and application state. From the example recording, our system automatically generates tutorials that illustrate the manipulation using images, text, and annotations. It leverages automated image labeling (recognition of facial features and outdoor scene structures in our implementation) to generate more precise text descriptions of many of the steps in the tutorials. A user study comparing our automatically generated tutorials to hand-designed tutorials and screencapture video recordings finds that users are 20\textendash 44\% faster and make 60\textendash 95\% fewer errors using our tutorials. While our system focuses on tutorial generation, we also present some initial work on generating content-dependent macros that use image recognition to automatically transfer selection operations from the example image used in the demonstration to new target images. While our macros are limited to transferring selection operations we demonstrate automatic transfer of several common retouching techniques including eye recoloring, whitening teeth and sunset enhancement.},
  file = {/Users/lukemurray/Zotero/storage/CTP8JNMP/Grabler et al. - 2009 - Generating photo manipulation tutorials by demonst.pdf;/Users/lukemurray/Zotero/storage/ZYC8B8SJ/Grabler et al. - 2009 - Generating photo manipulation tutorials by demonst.pdf},
  journal = {ACM Transactions on Graphics},
  language = {en},
  number = {3}
}

@inproceedings{grossmanChronicleCaptureExploration2010,
  title = {Chronicle: Capture, Exploration, and Playback of Document Workflow Histories},
  shorttitle = {Chronicle},
  booktitle = {Proceedings of the 23nd Annual {{ACM}} Symposium on {{User}} Interface Software and Technology - {{UIST}} '10},
  author = {Grossman, Tovi and Matejka, Justin and Fitzmaurice, George},
  year = {2010},
  pages = {143},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}},
  doi = {10.1145/1866029.1866054},
  abstract = {We describe Chronicle, a new system that allows users to explore document workflow histories. Chronicle captures the entire video history of a graphical document, and provides links between the content and the relevant areas of the history. Users can indicate specific content of interest, and see the workflows, tools, and settings needed to reproduce the associated results, or to better understand how it was constructed to allow for informed modification. Thus, by storing the rich information regarding the document's history workflow, Chronicle makes any working document a potentially powerful learning tool. We outline some of the challenges surrounding the development of such a system, and then describe our implementation within an image editing application. A qualitative user study produced extremely encouraging results, as users unanimously found the system both useful and easy to use.},
  file = {/Users/lukemurray/Zotero/storage/XXQB2TGU/Grossman et al. - 2010 - Chronicle capture, exploration, and playback of d.pdf},
  isbn = {978-1-4503-0271-5},
  language = {en}
}

@article{grossmanSurveySoftwareLearnability2009,
  title = {A {{Survey}} of {{Software Learnability}}: {{Metrics}}, {{Methodologies}} and {{Guidelines}}},
  author = {Grossman, Tovi and Fitzmaurice, George and Attar, Ramtin},
  year = {2009},
  pages = {10},
  abstract = {It is well-accepted that learnability is an important aspect of usability, yet there is little agreement as to how learnability should be defined, measured, and evaluated. In this paper, we present a survey of the previous definitions, metrics, and evaluation methodologies which have been used for software learnability. Our survey of evaluation methodologies leads us to a new question-suggestion protocol, which, in a user study, was shown to expose a significantly higher number of learnability issues in comparison to a more traditional think-aloud protocol. Based on the issues identified in our study, we present a classification system of learnability issues, and demonstrate how these categories can lead to guidelines for addressing the associated challenges.},
  file = {/Users/lukemurray/Zotero/storage/8J8C5X7C/Grossman et al. - 2009 - A Survey of Software Learnability Metrics, Method.pdf},
  language = {en}
}

@inproceedings{grossmanToolClipsInvestigationContextual2010,
  title = {{{ToolClips}}: An Investigation of Contextual Video Assistance for Functionality Understanding},
  shorttitle = {{{ToolClips}}},
  booktitle = {Proceedings of the 28th International Conference on {{Human}} Factors in Computing Systems - {{CHI}} '10},
  author = {Grossman, Tovi and Fitzmaurice, George},
  year = {2010},
  pages = {1515},
  publisher = {{ACM Press}},
  address = {{Atlanta, Georgia, USA}},
  doi = {10.1145/1753326.1753552},
  abstract = {We investigate the use of on-line contextual video assistance to improve the learnability of software functionality. After discussing motivations and design goals for such forms of assistance, we present our new technique, ToolClips. ToolClips augment traditional tooltips to provide users with quick and contextual access to both textual and video assistance. In an initial study we found that users successfully integrated ToolClip usage into the flow of their primary tasks to overcome learnability difficulties. In a second study, we found that with ToolClips, users successfully completed 7 times as many unfamiliar tasks, in comparison to using a commercial professionally developed on-line help system. Users also retained the information obtained from ToolClips, performing tasks significantly faster one week later.},
  file = {/Users/lukemurray/Zotero/storage/H6X5WWTI/Grossman and Fitzmaurice - 2010 - ToolClips an investigation of contextual video as.pdf},
  isbn = {978-1-60558-929-9},
  language = {en}
}

@article{hanschVideoOnlineLearning2015,
  title = {Video and {{Online Learning}}: {{Critical Reflections}} and {{Findings}} from the {{Field}}},
  shorttitle = {Video and {{Online Learning}}},
  author = {Hansch, Anna and Hillers, Lisa and McConachie, Katherine and Newman, Christopher and Schildhauer, Thomas and Schmidt, Philipp},
  year = {2015},
  issn = {1556-5068},
  doi = {10.2139/ssrn.2577882},
  file = {/Users/lukemurray/Zotero/storage/P7AFXEAE/Hansch et al. - 2015 - Video and Online Learning Critical Reflections an.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@article{hartmannHyperSourceBridgingGap2011,
  title = {{{HyperSource}}: Bridging the Gap between Source and Code-Related Web Sites},
  author = {Hartmann, Bj{\"o}rn and Dhillon, Mark and Chan, Matthew K},
  year = {2011},
  pages = {4},
  abstract = {Programmers frequently use the Web while writing code: they search for libraries, code examples, tutorials, and documentation. This link between code and visited Web pages remains implicit today. Connecting source code and browsing histories might help programmers maintain context, reduce the cost of Web page re-retrieval, and enhance understanding when code is shared. This note introduces HyperSource, an IDE augmentation that associates browsing histories with source code edits. HyperSource comprises a browser extension that logs visited pages; an IDE that tracks user activity and maps pages to code edits; a source document model that tracks visited pages at a character level; and a user interface that enables interaction with these histories. We discuss relevance heuristics and privacy issues inherent in this approach. Informal log analyses and user feedback suggest that our annotation model is promising for code editing and might also apply to other document authoring tasks after refinement.},
  file = {/Users/lukemurray/Zotero/storage/6WN8SAWK/Hartmann et al. - 2011 - HyperSource bridging the gap between source and c.pdf},
  language = {en}
}

@inproceedings{hillEditWearRead1992,
  title = {Edit Wear and Read Wear},
  booktitle = {Proceedings of the {{SIGCHI}} Conference on {{Human}} Factors in Computing Systems  - {{CHI}} '92},
  author = {Hill, William C. and Hollan, James D. and Wroblewski, Dave and McCandless, Tim},
  year = {1992},
  pages = {3--9},
  publisher = {{ACM Press}},
  address = {{Monterey, California, United States}},
  doi = {10.1145/142750.142751},
  abstract = {We describe two applications that illustrate the idea of computational wear in the domain of document processing. By graphically depicting the history of author and reader interactions with documents, these applications offer otherwise unavailable information to guide work. We discuss how their design accords with a theory of professional work and an informational physics perspective on interface design.},
  file = {/Users/lukemurray/Zotero/storage/46UARZU3/Hill et al. - 1992 - Edit wear and read wear.pdf},
  isbn = {978-0-89791-513-7},
  language = {en}
}

@inproceedings{huangGraphstractMinimalGraphical2007,
  title = {Graphstract: Minimal Graphical Help for Computers},
  shorttitle = {Graphstract},
  booktitle = {Proceedings of the 20th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology  - {{UIST}} '07},
  author = {Huang, Jeff and B. Twidale, Michael B.},
  year = {2007},
  pages = {203},
  publisher = {{ACM Press}},
  address = {{Newport, Rhode Island, USA}},
  doi = {10.1145/1294211.1294248},
  abstract = {We explore the use of abstracted screenshots as part of a new help interface. Graphstract, an implementation of a graphical help system, extends the ideas of textually oriented Minimal Manuals to the use of screenshots, allowing multiple small graphical elements to be shown in a limited space. This allows a user to get an overview of a complex sequential task as a whole. The ideas have been developed by three iterations of prototyping and evaluation. A user study shows that Graphstract helps users perform tasks faster on some but not all tasks. Due to their graphical nature, it is possible to construct Graphstracts automatically from pre-recorded interactions. A second study shows that automated capture and replay is a low-cost method for authoring Graphstracts, and the resultant help is as understandable as manually constructed help.},
  file = {/Users/lukemurray/Zotero/storage/6XXU3XBG/Huang and B. Twidale - 2007 - Graphstract minimal graphical help for computers.pdf},
  language = {en}
}

@article{johnsonAnimatedPedagogicalAgents,
  title = {Animated {{Pedagogical Agents}}: {{Face}}-to-{{Face Interaction}} in {{Interactive Learning Environments}}},
  author = {Johnson, W Lewis and Rickel, Jeff W and Lester, James C},
  pages = {32},
  abstract = {Recent years have witnessed the birth of a new paradigm for learning environments: animated pedagogical agents. These lifelike autonomous characters cohabit learning environments with students to create rich, face-to-face learning interactions. This opens up exciting new possibilities; for example, agents can demonstrate complex tasks, employ locomotion and gesture to focus students' attention on the most salient aspect of the task at hand, and convey emotional responses to the tutorial situation. Animated pedagogical agents offer great promise for broadening the bandwidth of tutorial communication and increasing learning environments' ability to engage and motivate students. This article sets forth the motivations behind animated pedagogical agents, describes the key capabilities they offer, and discusses the technical issues they raise. The discussion is illustrated with descriptions of a number of animated agents that represent the current state of the art.},
  file = {/Users/lukemurray/Zotero/storage/PKHMRQY4/Johnson et al. - Animated Pedagogical Agents Face-to-Face Interact.pdf},
  language = {en}
}

@inproceedings{kelleherStencilsbasedTutorialsDesign2005,
  title = {Stencils-Based Tutorials: Design and Evaluation},
  shorttitle = {Stencils-Based Tutorials},
  booktitle = {Proceedings of the {{SIGCHI}} Conference on {{Human}} Factors in Computing Systems  - {{CHI}} '05},
  author = {Kelleher, Caitlin and Pausch, Randy},
  year = {2005},
  pages = {541},
  publisher = {{ACM Press}},
  address = {{Portland, Oregon, USA}},
  doi = {10.1145/1054972.1055047},
  abstract = {Users of traditional tutorials and help systems often have difficulty finding the components described or pictured in the procedural instructions. Users also unintentionally miss steps, and perform actions that the documentation's authors did not intend, moving the application into an unknown state. We introduce Stencils, an interaction technique for presenting tutorials that uses translucent colored stencils containing holes that direct the user's attention to the correct interface component and prevent the user from interacting with other components. Sticky notes on the stencil's surface provide necessary tutorial material in the context of the application. In a user study comparing a Stencils-based and paper-based version of the same tutorial in Alice, a complex software application designed to teach introductory computer programming, we found that users of a Stencils-based tutorial were able complete the tutorial 26\% faster, with fewer errors, and less reliance on human assistance. Users of the Stencils-based and paper-based tutorials attained statistically similar levels of learning.},
  file = {/Users/lukemurray/Zotero/storage/89AUVPV9/Kelleher and Pausch - 2005 - Stencils-based tutorials design and evaluation.pdf},
  isbn = {978-1-58113-998-3},
  language = {en}
}

@article{khoussainovaSnipSuggestContextawareAutocompletion2010,
  title = {{{SnipSuggest}}: Context-Aware Autocompletion for {{SQL}}},
  shorttitle = {{{SnipSuggest}}},
  author = {Khoussainova, Nodira and Kwon, YongChul and Balazinska, Magdalena and Suciu, Dan},
  year = {2010},
  month = oct,
  volume = {4},
  pages = {22--33},
  issn = {21508097},
  doi = {10.14778/1880172.1880175},
  abstract = {In this paper, we present SnipSuggest, a system that provides onthe-go, context-aware assistance in the SQL composition process. SnipSuggest aims to help the increasing population of non-expert database users, who need to perform complex analysis on their large-scale datasets, but have difficulty writing SQL queries. As a user types a query, SnipSuggest recommends possible additions to various clauses in the query using relevant snippets collected from a log of past queries. SnipSuggest's current capabilities include suggesting tables, views, and table-valued functions in the FROM clause, columns in the SELECT clause, predicates in the WHERE clause, columns in the GROUP BY clause, aggregates, and some support for sub-queries. SnipSuggest adjusts its recommendations according to the context: as the user writes more of the query, it is able to provide more accurate suggestions.},
  file = {/Users/lukemurray/Zotero/storage/34WUPNG4/Khoussainova et al. - 2010 - SnipSuggest context-aware autocompletion for SQL.pdf},
  journal = {Proceedings of the VLDB Endowment},
  language = {en},
  number = {1}
}

@inproceedings{kimDatadrivenInteractionTechniques2014,
  title = {Data-Driven Interaction Techniques for Improving Navigation of Educational Videos},
  booktitle = {Proceedings of the 27th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology - {{UIST}} '14},
  author = {Kim, Juho and Guo, Philip J. and Cai, Carrie J. and Li, Shang-Wen (Daniel) and Gajos, Krzysztof Z. and Miller, Robert C.},
  year = {2014},
  pages = {563--572},
  publisher = {{ACM Press}},
  address = {{Honolulu, Hawaii, USA}},
  doi = {10.1145/2642918.2647389},
  abstract = {With an unprecedented scale of learners watching educational videos on online platforms such as MOOCs and YouTube, there is an opportunity to incorporate data generated from their interactions into the design of novel video interaction techniques. Interaction data has the potential to help not only instructors to improve their videos, but also to enrich the learning experience of educational video watchers. This paper explores the design space of data-driven interaction techniques for educational video navigation. We introduce a set of techniques that augment existing video interface widgets, including: a 2D video timeline with an embedded visualization of collective navigation traces; dynamic and non-linear timeline scrubbing; data-enhanced transcript search and keyword summary; automatic display of relevant still frames next to the video; and a visual summary representing points with high learner activity. To evaluate the feasibility of the techniques, we ran a laboratory user study with simulated learning tasks. Participants rated watching lecture videos with interaction data to be efficient and useful in completing the tasks. However, no significant differences were found in task performance, suggesting that interaction data may not always align with moment-by-moment information needs during the tasks.},
  file = {/Users/lukemurray/Zotero/storage/7SF5NJ5W/7SF5NJ5W.pdf},
  isbn = {978-1-4503-3069-5},
  language = {en}
}

@inproceedings{kimUnderstandingInvideoDropouts2014,
  title = {Understanding In-Video Dropouts and Interaction Peaks Inonline Lecture Videos},
  booktitle = {Proceedings of the First {{ACM}} Conference on {{Learning}} @ Scale Conference - {{L}}@{{S}} '14},
  author = {Kim, Juho and Guo, Philip J. and Seaton, Daniel T. and Mitros, Piotr and Gajos, Krzysztof Z. and Miller, Robert C.},
  year = {2014},
  pages = {31--40},
  publisher = {{ACM Press}},
  address = {{Atlanta, Georgia, USA}},
  doi = {10.1145/2556325.2566237},
  abstract = {With thousands of learners watching the same online lecture videos, analyzing video watching patterns provides a unique opportunity to understand how students learn with videos. This paper reports a large-scale analysis of in-video dropout and peaks in viewership and student activity, using second-by-second user interaction data from 862 videos in four Massive Open Online Courses (MOOCs) on edX. We find higher dropout rates in longer videos, re-watching sessions (vs first-time), and tutorials (vs lectures). Peaks in rewatching sessions and play events indicate points of interest and confusion. Results show that tutorials (vs lectures) and re-watching sessions (vs first-time) lead to more frequent and sharper peaks. In attempting to reason why peaks occur by sampling 80 videos, we observe that 61\% of the peaks accompany visual transitions in the video, e.g., a slide view to a classroom view. Based on this observation, we identify five student activity patterns that can explain peaks: starting from the beginning of a new material, returning to missed content, following a tutorial step, replaying a brief segment, and repeating a non-visual explanation. Our analysis has design implications for video authoring, editing, and interface design, providing a richer understanding of video learning on MOOCs.},
  file = {/Users/lukemurray/Zotero/storage/8AAQ7M99/Kim et al. - 2014 - Understanding in-video dropouts and interaction pe.pdf},
  isbn = {978-1-4503-2669-8},
  language = {en}
}

@inproceedings{klokmoseWebstratesShareableDynamic2015,
  title = {{\emph{Webstrates}}: {{Shareable Dynamic Media}}},
  shorttitle = {{\emph{Webstrates}}},
  booktitle = {Proceedings of the 28th {{Annual ACM Symposium}} on {{User Interface Software}} \& {{Technology}} - {{UIST}} '15},
  author = {Klokmose, Clemens N. and Eagan, James R. and Baader, Siemen and Mackay, Wendy and {Beaudouin-Lafon}, Michel},
  year = {2015},
  pages = {280--290},
  publisher = {{ACM Press}},
  address = {{Daegu, Kyungpook, Republic of Korea}},
  doi = {10.1145/2807442.2807446},
  abstract = {We revisit Alan Kay's early vision of dynamic media that blurs the distinction between documents and applications. We introduce shareable dynamic media that are malleable by users, who may appropriate them in idiosyncratic ways; shareable among users, who collaborate on multiple aspects of the media; and distributable across diverse devices and platforms. We present Webstrates, an environment for exploring shareable dynamic media. Webstrates augment web technology with real-time sharing. They turn web pages into substrates, i.e. software entities that act as applications or documents depending upon use. We illustrate Webstrates with two implemented case studies: users collaboratively author an article with functionally and visually different editors that they can personalize and extend at run-time; and they orchestrate its presentation and audience participation with multiple devices. We demonstrate the simplicity and generative power of Webstrates with three additional prototypes and evaluate it from a systems perspective.},
  file = {/Users/lukemurray/Zotero/storage/U58G4TGA/Klokmose et al. - 2015 - iWebstratesi Shareable Dynamic Media.pdf},
  isbn = {978-1-4503-3779-3},
  language = {en}
}

@inproceedings{kongDeltaToolRepresenting2012,
  title = {Delta: A Tool for Representing and Comparing Workflows},
  shorttitle = {Delta},
  booktitle = {Proceedings of the 2012 {{ACM}} Annual Conference on {{Human Factors}} in {{Computing Systems}} - {{CHI}} '12},
  author = {Kong, Nicholas and Grossman, Tovi and Hartmann, Bj{\"o}rn and Agrawala, Maneesh and Fitzmaurice, George},
  year = {2012},
  pages = {1027},
  publisher = {{ACM Press}},
  address = {{Austin, Texas, USA}},
  doi = {10.1145/2207676.2208549},
  abstract = {Tutorials and sample workflows for complicated, featurerich software packages are widely available online. As a result users must differentiate between workflows to choose the most suitable one for their task. We present Delta, an interactive workflow visualization and comparison tool that helps users identify the tradeoffs between workflows. We conducted an initial study to identify the set of attributes users attend to when comparing workflows, finding that they consider result quality, their knowledge of commands, and the efficiency of the workflow. We then designed Delta to surface these attributes at three granularities: a highlevel, clustered view; an intermediate-level list view that contains workflow summaries; and a low-level detail view that allows users to compare two individual workflows. Finally, we conducted an evaluation of Delta on a small corpus of 30 workflows and found that the intermediate list view provided the best information density. We conclude with thoughts on how such a workflow comparison system could be scaled up to larger corpora in the future.},
  file = {/Users/lukemurray/Zotero/storage/QCSZP93M/QCSZP93M.pdf},
  isbn = {978-1-4503-1015-4},
  language = {en}
}

@inproceedings{kurlanderHistorybasedMacroExample1992,
  title = {A History-Based Macro by Example System},
  booktitle = {Proceedings of the 5th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology  - {{UIST}} '92},
  author = {Kurlander, David and Feiner, Steven},
  year = {1992},
  pages = {99--106},
  publisher = {{ACM Press}},
  address = {{Monteray, California, United States}},
  doi = {10.1145/142621.142633},
  abstract = {Many tasks performed using computer interfaces are very repetitive. While programmers can write macros or procedures to automate these repetitive tasks, this requires special skills. Demonstrational systems make macro building accessible to all users, but most provide either no visual representation of the macro or only a textual representation. We have developed a history-based visual representation of commands in a graphical user interface. This representation supports the definition of macros by example in several novel ways. At any time, a user can open a history window, review the commands executed in a session, select operations to encapsulate into a macro, and choose objects and their attributes as arguments. The system has facilities to generalize the macro automatically, save it for future use, and edit it.},
  file = {/Users/lukemurray/Zotero/storage/YBHXFVBA/Kurlander and Feiner - 1992 - A history-based macro by example system.pdf},
  isbn = {978-0-89791-549-6},
  language = {en}
}

@article{kurlanderVisualLanguageBrowsing1989,
  title = {A {{Visual Language}} for {{Browsing}}, {{Undoing}}, and {{Redoing Graphical Interface Commands}}},
  author = {Kurlander, David and Feiner, Steven K.},
  year = {1989},
  doi = {10.7916/D8W95NFS},
  abstract = {We present the concept of an editable graphical history that allows the user to review and modify the actions performed with a graphical user interface. Using a pictorial metaphor borrowed from comic strips, an editable graphical history consists of a series of panels that depict in chronological order the important events in the history of a user's session. We discuss the visual language used in editable graphical histories, and describe Chimera, a graphical editor that generates these histories automatically. The user may scroll through the sequence of panels, reviewing actions at different levels of detail, and selectively undoing, modifying, and redoing previous actions. Chimera's editable graphical histories are constructed from parts of the editor window, the editor control panel, and the editor's pop up menus. Panels indicate both the objects that are modified and the actions performed on them. We describe the heuristics used to determine the objects depicted in each panel, the style in which they are drawn, and how actions are distributed among panels.},
  file = {/Users/lukemurray/Zotero/storage/8DGA28FL/Kurlander and Feiner - 1989 - A Visual Language for Browsing, Undoing, and Redoi.pdf;/Users/lukemurray/Zotero/storage/AKMPXURA/D8W95NFS.html},
  language = {en}
}

@inproceedings{lafreniereCommunityEnhancedTutorials2013,
  title = {Community Enhanced Tutorials: Improving Tutorials with Multiple Demonstrations},
  shorttitle = {Community Enhanced Tutorials},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}} - {{CHI}} '13},
  author = {Lafreniere, Benjamin and Grossman, Tovi and Fitzmaurice, George},
  year = {2013},
  pages = {1779},
  publisher = {{ACM Press}},
  address = {{Paris, France}},
  doi = {10.1145/2470654.2466235},
  abstract = {Web-based tutorials are a popular help resource for learning how to perform unfamiliar tasks in complex software. However, in their current form, web tutorials are isolated from the applications that they support. In this paper we present FollowUs, a web-tutorial system that integrates a fully-featured application into a web-based tutorial. This novel architecture enables community enhanced tutorials, which continuously improve as more users work with them. FollowUs captures video demonstrations of users as they perform a tutorial. Subsequent users can use the original tutorial, or choose from a library of captured community demonstrations of each tutorial step. We conducted a user study to test the benefits of making multiple demonstrations available to users, and found that users perform significantly better using our system with a library of multiple demonstrations in comparison to its equivalent baseline system with only the original authored content.},
  file = {/Users/lukemurray/Zotero/storage/YPJR3FXI/Lafreniere et al. - 2013 - Community enhanced tutorials improving tutorials .pdf},
  isbn = {978-1-4503-1899-0},
  language = {en}
}

@inproceedings{liebermanMondrianTeachableGraphical1993,
  title = {Mondrian: A Teachable Graphical Editor},
  shorttitle = {Mondrian},
  booktitle = {Proceedings of the {{SIGCHI}} Conference on {{Human}} Factors in Computing Systems  - {{CHI}} '93},
  author = {Lieberman, Henry},
  year = {1993},
  pages = {144},
  publisher = {{ACM Press}},
  address = {{Amsterdam, The Netherlands}},
  doi = {10.1145/169059.169120},
  file = {/Users/lukemurray/Zotero/storage/RZ74S2BM/Lieberman - 1993 - Mondrian a teachable graphical editor.pdf},
  isbn = {978-0-89791-575-5},
  language = {en}
}

@article{matuschakHowCanWe2019,
  title = {How Can We Develop Transformative Tools for Thought?},
  author = {Matuschak, Andy and Nielsen, Michael and Matuschak, Andy and Nielsen, Michael},
  year = {2019},
  abstract = {},
  file = {/Users/lukemurray/Zotero/storage/TQ66QYBS/ttft.html}
}

@article{matuschakQuantumCountry2019,
  title = {Quantum {{Country}}},
  author = {Matuschak, Andy and Nielsen, Michael},
  year = {2019},
  abstract = {A free introduction to quantum computing and quantum mechanics},
  file = {/Users/lukemurray/Zotero/storage/VATFHRSC/quantum.country.html}
}

@article{milesMoreThisQuery,
  title = {More {{Like This}}: {{Query Recommendations}} for {{SQL}}},
  author = {Miles, Christopher},
  pages = {9},
  file = {/Users/lukemurray/Zotero/storage/ACCBCTX6/Miles - More Like This Query Recommendations for SQL.pdf},
  language = {en}
}

@article{mintzOpinionWhyLearning2020,
  title = {Opinion | {{Why I}}'m {{Learning More With Distance Learning Than I Do}} in {{School}}},
  author = {Mintz, Veronique},
  year = {2020},
  month = may,
  issn = {0362-4331},
  abstract = {I'm 13 years old. I don't miss the other kids who talk out of turn, disrespect teachers and hit one another.},
  chapter = {Opinion},
  journal = {The New York Times},
  keywords = {Computers and the Internet,Coronavirus (2019-nCoV),E-Learning,Education (K-12),Manhattan (NYC),New York City,School Discipline (Students),Teachers and School Employees,Upper West Side (Manhattan; NY)},
  language = {en-US}
}

@article{morenoInteractiveMultimodalLearning2007,
  title = {Interactive {{Multimodal Learning Environments}}: {{Special Issue}} on {{Interactive Learning Environments}}: {{Contemporary Issues}} and {{Trends}}},
  shorttitle = {Interactive {{Multimodal Learning Environments}}},
  author = {Moreno, Roxana and Mayer, Richard},
  year = {2007},
  month = sep,
  volume = {19},
  pages = {309--326},
  issn = {1040-726X, 1573-336X},
  doi = {10.1007/s10648-007-9047-2},
  abstract = {What are interactive multimodal learning environments and how should they be designed to promote students' learning? In this paper, we offer a cognitive\textendash affective theory of learning with media from which instructional design principles are derived. Then, we review a set of experimental studies in which we found empirical support for five design principles: guided activity, reflection, feedback, control, and pretraining. Finally, we offer directions for future instructional technology research.},
  file = {/Users/lukemurray/Zotero/storage/P2X8EKMQ/P2X8EKMQ.pdf},
  journal = {Educational Psychology Review},
  language = {en},
  number = {3}
}

@inproceedings{mysorePortaProfilingSoftware2018,
  title = {Porta: {{Profiling Software Tutorials Using Operating}}-{{System}}-{{Wide Activity Tracing}}},
  shorttitle = {Porta},
  booktitle = {Proceedings of the 31st {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Mysore, Alok and Guo, Philip J.},
  year = {2018},
  month = oct,
  pages = {201--212},
  publisher = {{Association for Computing Machinery}},
  address = {{Berlin, Germany}},
  doi = {10.1145/3242587.3242633},
  abstract = {It can be hard for tutorial creators to get fine-grained feedback about how learners are actually stepping through their tutorials and which parts lead to the most struggle. To provide such feedback for technical software tutorials, we introduce the idea of tutorial profiling, which is inspired by software code profiling. We prototyped this idea in a system called Porta that automatically tracks how users navigate through a tutorial webpage and what actions they take on their computer such as running shell commands, invoking compilers, and logging into remote servers. Porta surfaces this trace data in the form of profiling visualizations that augment the tutorial with heatmaps of activity hotspots and markers that expand to show event details, error messages, and embedded screencast videos of user actions. We found through a user study of 3 tutorial creators and 12 students who followed their tutorials that Porta enabled both the tutorial creators and the students to provide more specific, targeted, and actionable feedback about how to improve these tutorials. Porta opens up possibilities for performing user testing of technical documentation in a more systematic and scalable way.},
  file = {/Users/lukemurray/Zotero/storage/I5Y6CS5B/I5Y6CS5B.pdf},
  isbn = {978-1-4503-5948-1},
  keywords = {activity tracing,software tutorials,tutorial profiling},
  series = {{{UIST}} '18}
}

@inproceedings{mysoreTortaGeneratingMixedMedia2017,
  title = {Torta: {{Generating Mixed}}-{{Media GUI}} and {{Command}}-{{Line App Tutorials Using Operating}}-{{System}}-{{Wide Activity Tracing}}},
  shorttitle = {Torta},
  booktitle = {Proceedings of the 30th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Mysore, Alok and Guo, Philip J.},
  year = {2017},
  month = oct,
  pages = {703--714},
  publisher = {{ACM}},
  address = {{Qu\'ebec City QC Canada}},
  doi = {10.1145/3126594.3126628},
  abstract = {Tutorials are vital for helping people perform complex software-based tasks in domains such as programming, data science, system administration, and computational research. However, it is tedious to create detailed step-by-step tutorials for tasks that span multiple interrelated GUI and commandline applications. To address this challenge, we created Torta, an end-to-end system that automatically generates step-bystep GUI and command-line app tutorials by demonstration, provides an editor to trim, organize, and add validation criteria to these tutorials, and provides a web-based viewer that can validate step-level progress and automatically run certain steps. The core technical insight that underpins Torta is that combining operating-system-wide activity tracing and screencast recording makes it easier to generate mixed-media (text+video) tutorials that span multiple GUI and commandline apps. An exploratory study on 10 computer science teaching assistants (TAs) found that they all preferred the experience and results of using Torta to record programming and sysadmin tutorials relevant to classes they teach rather than manually writing tutorials. A follow-up study on 6 students found that they all preferred following the Torta tutorials created by those TAs over the manually-written versions.},
  file = {/Users/lukemurray/Zotero/storage/5TMB2MRR/Mysore and Guo - 2017 - Torta Generating Mixed-Media GUI and Command-Line.pdf},
  isbn = {978-1-4503-4981-9},
  language = {en}
}

@inproceedings{nakamuraApplicationindependentSystemVisualizing2008,
  ids = {nakamuraApplicationindependentSystemVisualizing2008a},
  title = {An Application-Independent System for Visualizing User Operation History},
  booktitle = {Proceedings of the 21st Annual {{ACM}} Symposium on {{User}} Interface Software and Technology - {{UIST}} '08},
  author = {Nakamura, Toshio and Igarashi, Takeo},
  year = {2008},
  pages = {23},
  publisher = {{ACM Press}},
  address = {{Monterey, CA, USA}},
  doi = {10.1145/1449715.1449721},
  abstract = {A history-of-user-operations function helps make applications easier to use. For example, users may have access to an operation history list in an application to undo or redo a past operation. To provide an overview of a long operation history and help users find target interactions or application states quickly, visual representations of operation history have been proposed. However, most previous systems are tightly integrated with target applications and difficult to apply to new applications. We propose an applicationindependent method that can visualize the operation history of arbitrary GUI applications by monitoring the input and output GUI events from outside of the target application. We implemented a prototype system that visualizes operation sequences of generic Java Awt/Swing applications using an annotated comic strip metaphor. We tested the system with various applications and present results from a user study.},
  file = {/Users/lukemurray/Zotero/storage/44Z9QVR7/Nakamura and Igarashi - 2008 - An application-independent system for visualizing .pdf;/Users/lukemurray/Zotero/storage/BHPL55SU/Nakamura and Igarashi - 2008 - An application-independent system for visualizing .pdf},
  isbn = {978-1-59593-975-3},
  language = {en}
}

@inproceedings{nandiAssistedQueryingUsing2007,
  title = {Assisted Querying Using Instant-Response Interfaces},
  booktitle = {Proceedings of the 2007 {{ACM SIGMOD}} International Conference on {{Management}} of Data  - {{SIGMOD}} '07},
  author = {Nandi, Arnab and Jagadish, H. V.},
  year = {2007},
  pages = {1156},
  publisher = {{ACM Press}},
  address = {{Beijing, China}},
  doi = {10.1145/1247480.1247640},
  abstract = {We demonstrate a novel query interface that enables users to construct a rich search query without any prior knowledge of the underlying schema or data. The interface, which is in the form of a single text input box, interacts in real-time with the users as they type, guiding them through the query construction. We discuss the issues of schema and data complexity, result size estimation, and query validity; and provide novel approaches to solving these problems. We demonstrate our query interface on two popular applications; an enterprise-wide personnel search, and a biological information database.},
  file = {/Users/lukemurray/Zotero/storage/XPJ4Q7AG/Nandi and Jagadish - 2007 - Assisted querying using instant-response interface.pdf},
  isbn = {978-1-59593-686-8},
  language = {en}
}

@book{nandiGuidedInteractionRethinking,
  title = {Guided {{Interaction}}: {{Rethinking}} the {{Query}}-{{Result Paradigm ABSTRACT}}},
  shorttitle = {Guided {{Interaction}}},
  author = {Nandi, Arnab},
  abstract = {Many decades of research, coupled with continuous increases in computing power, have enabled highly efficient execution of queries on large databases. In consequence, for many databases, far more time is spent by users formulating queries than by the system evaluating them. It stands to reason that, looking at the overall query experience we provide users, we should pay attention to how we can assist users in the holistic process of obtaining the information they desire from the database, and not just the constituent activity of efficiently generating a result given a complete precise query. In this paper, we examine the conventional query-result paradigm employed by databases and demonstrate challenges encountered when following this paradigm for an information seeking task. We recognize that the process of query specification itself is a major stumbling block. With current computational abilities, we are at a point where we can make use of the data in the database to aid in this process. To this end, we propose a new paradigm, guided interaction, to solve the noted challenges, by using interaction to guide the user through the query formulation, query execution and result examination processes. The user can be given advance information during query specification that can not only assist in query formulation, but may also lead to abandonment of an unproductive query direction or the satisfaction of information need even before the query specification is complete. There are significant engineering challenges to constructing the system we envision, and the technological building blocks to address these challenges exist today. 1.},
  file = {/Users/lukemurray/Zotero/storage/P2UTAIC4/Nandi - Guided Interaction Rethinking the Query-Result Pa.pdf;/Users/lukemurray/Zotero/storage/UMLJJ6GN/summary.html}
}

@inproceedings{nguyenMakingSoftwareTutorial2015,
  title = {Making {{Software Tutorial Video Responsive}}},
  booktitle = {Proceedings of the 33rd {{Annual ACM Conference}} on {{Human Factors}} in {{Computing Systems}} - {{CHI}} '15},
  author = {Nguyen, Cuong and Liu, Feng},
  year = {2015},
  pages = {1565--1568},
  publisher = {{ACM Press}},
  address = {{Seoul, Republic of Korea}},
  doi = {10.1145/2702123.2702209},
  file = {/Users/lukemurray/Zotero/storage/HTKDZA7H/Nguyen and Liu - 2015 - Making Software Tutorial Video Responsive.pdf},
  isbn = {978-1-4503-3145-6},
  language = {en}
}

@misc{pressNurnbergFunnelMIT,
  title = {The {{Nurnberg Funnel}} | {{The MIT Press}}},
  author = {Press, The MIT},
  publisher = {{The MIT Press}},
  abstract = {The legendary Funnel of Nurnberg was said to make people wise very quickly when the right knowledge was poured in; it is an approach that designers continue to apply in trying to make instruction more efficient. 
                How do people acquire beginning competence at using new technology? The legendary Funnel of Nurnberg was said to make people wise very quickly when the right knowledge was poured in; it is an approach that designers continue to apply in trying to make instruction more efficient. This book describes a quite different instructional paradigm that uses what learners do spontaneously to find meaning in the activities of learning. It presents the "minimalist" approach to instructional design - its origins in the study of people's learning problems with computer systems, its foundations in the psychology of learning and problem solving, and its application in a variety of case studies. Carroll demonstrates that the minimalist approach outperforms the standard "systems approach" in every relevant way - the learner, not the system determines the model and the methods of instruction. It supports the rapid achievement of realistic projects right from the start of training, instead of relying on drill and practice techniques, and designing for error recognition and recovery as basic instructional events, instead of seeing error as failure. The book's many examples - including a brief discussion of recent commercial applications - will help researchers and practitioners apply and develop this new instructional technology.John M. Carroll has participated for a number of years as a leader in the interdisciplinary field of human-computer interactions. He is Manager of User Interface Theory and Design at IBM's Watson Research Center. The Nurnberg Funnel inaugurates the Technical Communications series, edited by Ed Barrett.},
  file = {/Users/lukemurray/Zotero/storage/H5XXF4FQ/nurnberg-funnel.html},
  howpublished = {https://mitpress.mit.edu/books/nurnberg-funnel},
  language = {en}
}

@article{rameshShowMeHowTranslatingUser2011,
  title = {{{ShowMeHow}}: {{Translating User Interface Instructions Between Similar Applications}}},
  author = {Ramesh, Vidya and Hsu, Charlie and Agrawala, Maneesh and Hartmann, Bjorn},
  year = {2011},
  pages = {8},
  abstract = {Many people learn how to use complex authoring applications through tutorials. However, user interfaces for authoring tools differ between versions, platforms, and competing products, limiting the utility of tutorials. Our goal is to make tutorials more useful by enabling users to repurpose tutorials between similar applications. We introduce UI translation interfaces which enable users to locate commands in one application using the interface language of another application. Our end-user tool, ShowMeHow, demonstrates two interaction techniques to accomplish translations: 1) direct manipulation of interface fac\c ades and 2) text search for commands using the vocabulary of another application. We discuss tools needed to construct the translation maps that enable these techniques. An initial study (n=12) shows that users can locate unfamiliar commands twice as fast with interface fac\c ades. A second study shows that ShowMeHow enables users to work through tutorials written for one application in another application.},
  file = {/Users/lukemurray/Zotero/storage/D8L34K49/Ramesh et al. - 2011 - ShowMeHow Translating User Interface Instructions.pdf},
  journal = {Social Learning},
  language = {en}
}

@inproceedings{wangMismatchExpectationsHow2018,
  title = {Mismatch of {{Expectations}}: {{How Modern Learning Resources Fail Conversational Programmers}}},
  shorttitle = {Mismatch of {{Expectations}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}  - {{CHI}} '18},
  author = {Wang, April Y. and Mitts, Ryan and Guo, Philip J. and Chilana, Parmit K.},
  year = {2018},
  pages = {1--13},
  publisher = {{ACM Press}},
  address = {{Montreal QC, Canada}},
  doi = {10.1145/3173574.3174085},
  abstract = {Conversational programmers represent a class of learners who are not required to write any code, yet try to learn programming to improve their participation in technical conversations. We carried out interviews with 23 conversational programmers to better understand the challenges they face in technical conversations, what resources they choose to learn programming, how they perceive the learning process, and to what extent learning programming actually helps them. Among our key findings, we found that conversational programmers often did not know where to even begin the learning process and ended up using formal and informal learning resources that focus largely on programming syntax and logic. However, since the end goal of conversational programmers was not to build artifacts, modern learning resources usually failed these learners in their pursuits of improving their technical conversations. Our findings point to design opportunities in HCI to invent learnercentered approaches that address the needs of conversational programmers and help them establish common ground in technical conversations.},
  file = {/Users/lukemurray/Zotero/storage/5IUCSKJX/Wang et al. - 2018 - Mismatch of Expectations How Modern Learning Reso.pdf},
  isbn = {978-1-4503-5620-6},
  language = {en}
}

@inproceedings{yehSikuliUsingGUI2009,
  title = {Sikuli: Using {{GUI}} Screenshots for Search and Automation},
  shorttitle = {Sikuli},
  booktitle = {Proceedings of the 22nd Annual {{ACM}} Symposium on {{User}} Interface Software and Technology - {{UIST}} '09},
  author = {Yeh, Tom and Chang, Tsung-Hsiang and Miller, Robert C.},
  year = {2009},
  pages = {183},
  publisher = {{ACM Press}},
  address = {{Victoria, BC, Canada}},
  doi = {10.1145/1622176.1622213},
  abstract = {We present Sikuli, a visual approach to search and automation of graphical user interfaces using screenshots. Sikuli allows users to take a screenshot of a GUI element (such as a toolbar button, icon, or dialog box) and query a help system using the screenshot instead of WKH  HOHPHQW\textparagraph V  name. Sikuli also provides a visual scripting API for automating GUI interactions, using screenshot patterns to direct mouse and keyboard events. We report a web-based user study showing that searching by screenshot is easy to learn and faster to specify than keywords. We also demonstrate several automation tasks suitable for visual scripting, such as map navigation and bus tracking, and show how visual scripting can improve interactive help systems previously proposed in the literature.},
  file = {/Users/lukemurray/Zotero/storage/ACBIIYEJ/Yeh et al. - 2009 - Sikuli using GUI screenshots for search and autom.pdf},
  isbn = {978-1-60558-745-5},
  language = {en}
}


